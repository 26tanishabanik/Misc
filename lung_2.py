# -*- coding: utf-8 -*-
"""Lung-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MWhC6r5F3C80kO9H35SDE24a1Boss3I3
"""

from google.colab import drive
drive.mount('/content/gdrive/',force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My Drive/Lung Nodules

with tf.device('/device:GPU:0'):
  ! python train_segmentation.py

import tensorflow as tf
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self,epoch,logs= {}):
    if(logs.get('acc')>0.98):
      print('reached {}% accuracy on training set and {}% accuracy on test set'.format(logs.get('acc'), logs.get('val_acc')))
      self.model.stop_training = True

callback = myCallback()

# Define the model
model = Sequential()

#1st convolution layer
model.add(Conv2D(16, (3, 3), padding='same', input_shape=(128,128,3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2), padding='same'))

#2nd convolution layer
model.add(Conv2D(2,(3, 3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2), padding='same'))
#-------------------------
#3rd convolution layer
model.add(Conv2D(2,(3, 3), padding='same'))
model.add(Activation('relu'))
model.add(UpSampling2D((2, 2)))

#4rd convolution layer
model.add(Conv2D(16,(3, 3), padding='same'))
model.add(Activation('relu'))
model.add(UpSampling2D((2, 2)))

#-------------------------

model.add(Conv2D(3,(3, 3), padding='same'))
model.add(Activation('sigmoid'))

model.summary()

# Compile the model
model.compile(optimizer='adadelta', loss='binary_crossentropy')

# Generate data from the images in a folder
batch_size = 8
train_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last', validation_split = 0.1)
train_generator = train_datagen.flow_from_directory(
    'cropped/',
    target_size=(128, 128),
    batch_size=batch_size,
    class_mode='input',
    subset = 'training'
    )
validation_generator = train_datagen.flow_from_directory(
    'cropped/',
    target_size=(128, 128),
    batch_size=batch_size,
    class_mode='input',
    subset = 'validation',
    )
    
# Train the model
model.fit_generator(
        train_generator,
        steps_per_epoch=1000 // batch_size,
        epochs=20,
        validation_data=validation_generator,
        validation_steps=1000 // batch_size)
        

'''# Test the model
data_list = []
batch_index = 0
while batch_index <= train_generator.batch_index:
    data = train_generator.next()
    data_list.append(data[0])
    batch_index = batch_index + 1
data_list[0].shape

predicted = model.predict(data_list[0])
plt.imshow(data_list[0][0])
plt.imshow(predicted[0])'''

#Encoder
import keras
from keras import layers
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.image import img_to_array
from PIL import Image
import os
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten, Input
from keras.layers import Conv2D, MaxPooling2D, UpSampling2D
import matplotlib.pyplot as plt
from keras import backend as K
import numpy as np
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img

def train_autoEn(batch_size = 16):
  path = '/content/gdrive/My Drive/Lung Nodules/data_1'
  '''train_image_generator = ImageDataGenerator(
      rescale = 1./255,
      #rotation_range=45,
      #width_shift_range=0.1,
      #height_shift_range=0.1,
      #shear_range=0.1,
      #zoom_range=0.0,
      #fill_mode="nearest",
      #horizontal_flip=True,
      #vertical_flip=True,
      #validation_split = 0.1
    ) 
  train_data = train_image_generator.flow_from_directory(batch_size = batch_size,
                                                           directory = path, 
                                                           #shuffle = True,
                                                           target_size = (128, 128),
                                                           class_mode = 'input',
                                                           subset = 'training',
                                                           seed = 2)
  validation_data = train_image_generator.flow_from_directory(batch_size = batch_size,
                                                           directory = path, 
                                                           #shuffle = True,
                                                           target_size = (128, 128),
                                                           class_mode = 'input',
                                                           subset = 'validation',
                                                           seed = 2)
  imgs = os.listdir(path)
  train_data = [0]*223 #np.zeros([223,128,128,1], dtype=np.float)
  test_data = [0]*24 #np.zeros([24,128,128,1], dtype=np.float)
  count = 0
  for img in imgs:
    image = Image.open(path + '/' + img)
    image = image.resize((128, 128))
    if count < 223:
      train_data[count] = img_to_array(image).reshape(128, 128, 1)/255
      count += 1
    else:
      test_data[count - 223] = img_to_array(image).reshape(128, 128, 1)/255
      count += 1


  input_img = keras.Input(shape=(128, 128, 1))

  x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
  x = layers.MaxPooling2D((2, 2), padding='same')(x)
  x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)
  x = layers.MaxPooling2D((2, 2), padding='same')(x)
  x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)
  encoded = layers.MaxPooling2D((2, 2), padding='same')(x)
  # at this point the representation is (4, 4, 8) i.e. 128-dimensional
  x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)
  x = layers.UpSampling2D((2, 2))(x)
  x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)
  x = layers.UpSampling2D((2, 2))(x)
  x = layers.Conv2D(32, (3, 3), activation='relu')(x)
  x = layers.UpSampling2D((2, 2))(x)
  decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)

  autoencoder = keras.Model(input_img, decoded)
  autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['acc'])
  history = autoencoder.fit(train_data, steps_per_epoch = train_data.samples//batch_size,  validation_data = validation_data, validation_steps = validation_data.samples//batch_size, epochs = 10000, callbacks = [callback])'''
  # Define the model
  model = Sequential()

  #1st convolution layer
  model.add(Conv2D(16, (3, 3), padding='same', input_shape=(128,128,3)))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2,2), padding='same'))

  #2nd convolution layer
  model.add(Conv2D(2,(3, 3), padding='same'))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2,2), padding='same'))
  #-------------------------
  #3rd convolution layer
  model.add(Conv2D(2,(3, 3), padding='same'))
  model.add(Activation('relu'))
  model.add(UpSampling2D((2, 2)))

  #4rd convolution layer
  model.add(Conv2D(16,(3, 3), padding='same'))
  model.add(Activation('relu'))
  model.add(UpSampling2D((2, 2)))

  #-------------------------

  model.add(Conv2D(3,(3, 3), padding='same'))
  model.add(Activation('sigmoid'))

  model.summary()

  # Compile the model
  model.compile(optimizer='adadelta', loss='binary_crossentropy')

  # Generate data from the images in a folder
  batch_size = 8
  train_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last') #, validation_split = 0.1)
  train_generator = train_datagen.flow_from_directory(
      path,
      target_size=(128, 128),
      batch_size=batch_size,
      class_mode='input'
      )
  '''validation_generator = train_datagen.flow_from_directory(
      path,
      target_size=(128, 128),
      batch_size=batch_size,
      class_mode='input',
      subset = 'validation',
      )'''
  trainauto = train_generator   
  #validauto = validation_generator
  # Train the model
  history = model.fit_generator(
              train_generator,
              steps_per_epoch=train_generator.samples // batch_size,
              epochs=10000, callbacks = [callback])
              #validation_data=validation_generator,
              #validation_steps=validation_generator.samples // batch_size)
  hist_df = pd.DataFrame(history.history)  
  hist_json_file = 'autoEnhistory.json' 
  with open(hist_json_file, mode='w') as f:
    hist_df.to_json(f)
  autoencoder.save('autoEn.h5')
  print('done training AutoEn')

import numpy as np
def train_NN():
  encoder_arc = tf.keras.models.load_model('autoEn.h5')
  #encoder = keras.Input(encoder_arc[0])
  #enc_op = keras.layers.Flatten()(encoder_arc[-1])
  endocer = keras.Model(encoder_arc.input, encoder_arc.layers[-6].output)
  CNN = tf.keras.models.load_model('CNN.h5')
  train_image_generator = ImageDataGenerator(
      rescale = 1./255,
      #rotation_range=45,
      #width_shift_range=0.1,
      #height_shift_range=0.1,
      #shear_range=0.1,
      #zoom_range=0.0,
      #fill_mode="nearest",
      #horizontal_flip=True,
      #vertical_flip=True,
      validation_split = 0.1
    ) 
  train_data = train_image_generator.flow_from_directory(batch_size = 223,
                                                           directory = '/content/gdrive/My Drive/Lung Nodules/new_data', 
                                                           shuffle = True,
                                                           target_size = (128, 128),
                                                           class_mode = 'categorical',
                                                           subset = 'training',
                                                           seed = 2)
  validation_data = train_image_generator.flow_from_directory(batch_size = 24,
                                                           directory = '/content/gdrive/My Drive/Lung Nodules/new_data', 
                                                           shuffle = True,
                                                           target_size = (128, 128),
                                                           class_mode = 'categorical',
                                                           subset = 'validation',
                                                           seed = 2)
  prediction_en = encoder.predict(trainauto)
  en_arr = np.array(prediction_en, dtype=np.float).reshape(223, 768)/255
  prediction_cnn = CNN.predict(train_data)
  cnn_arr = np.array(prediction_cnn, dtype=np.float).reshape(223, 1)/2
  input_nn = np.concatenate((en_arr, cnn_arr), axis=1)
  '''prediction_en = encoder.predict(validauto)
  en_arr = np.array(prediction_en, dtype=np.float).reshape(24, 256)/255
  prediction_cnn = CNN.predict(validation_data)
  cnn_arr = np.array(prediction_cnn, dtype=np.float).reshape(24, 1)/2
  validation_nn = np.concatenate((en_arr, cnn_arr), axis=1)'''




  model = tf.keras.models.sequential([
                                      tf.keras.layers.Dense(769, activation = 'relu'),
                                      tf.keras.layers.Dense(128, activation = 'relu'),
                                      tf.keras.layers.Dense(64, activation = 'relu'),
                                      tf.keras.layers.Dense(1, activation = 'relu')                                  
  ])
  model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['acc', 'Precision', 'Recall',tf.keras.metrics.SpecificityAtSensitivity(0.98), tf.keras.metrics.SensitivityAtSpecificity(0.98), 'TruePositives', 'TrueNegatives', 'AUC'])
  history = model.fit(input_nn, steps_per_epoch = 247)#, validation_data = validation_nn, validation_steps = 24, epochs = 10000, callbacks = [callback])
  hist_df = pd.DataFrame(history.history)  
  hist_json_file = 'NNhistory.json' 
  with open(hist_json_file, mode='w') as f:
    hist_df.to_json(f)
  model.save('NN.h5')
  print('done training DNN')

#CNN
def train_CNN(batch_size=16):
  train_image_generator = ImageDataGenerator(
      rescale = 1./255,
      #rotation_range=45,
      #width_shift_range=0.1,
      #height_shift_range=0.1,
      #shear_range=0.1,
      #zoom_range=0.0,
      #fill_mode="nearest",
      #horizontal_flip=True,
      #vertical_flip=True,
      validation_split = 0.1
    ) 
  train_data = train_image_generator.flow_from_directory(batch_size = batch_size,
                                                           directory = '/content/gdrive/My Drive/Lung Nodules/new_data', 
                                                           shuffle = True,
                                                           target_size = (128, 128),
                                                           class_mode = 'categorical',
                                                           subset = 'training',
                                                           seed = 2)
  validation_data = train_image_generator.flow_from_directory(batch_size = batch_size,
                                                           directory = '/content/gdrive/My Drive/Lung Nodules/new_data', 
                                                           shuffle = True,
                                                           target_size = (128, 128),
                                                           class_mode = 'categorical',
                                                           subset = 'validation',
                                                           seed = 2)
  model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(8, (3,3),activation = 'relu', input_shape = [128,128 , 1]),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(16, (3,3), activation= 'relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(32, (3,3), activation= 'relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    #tf.keras.layers.Conv2D(64, (3,3), activation= 'relu'),
    #tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation = 'relu'),
    tf.keras.layers.Dense(64, activation = 'relu'),
    tf.keras.layers.Dense(3, activation = 'softmax'),  
  ])
  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc', 'Precision', 'Recall',tf.keras.metrics.SpecificityAtSensitivity(0.98), tf.keras.metrics.SensitivityAtSpecificity(0.98), 'TruePositives', 'TrueNegatives', 'AUC'])
  history = model.fit(train_data, steps_per_epoch = train_data.samples//batch_size, validation_data = validation_data, validation_steps = validation_data.samples//batch_size, epochs = 10000, callbacks = [callback])
  hist_df = pd.DataFrame(history.history)  
  hist_json_file = 'CNNhistory.json' 
  with open(hist_json_file, mode='w') as f:
    hist_df.to_json(f)
  model.save('CNN.h5')
  print('done training CNN')

#train_autoEn(batch_size = 16)
train_CNN(batch_size=16)
#train_NN()

from matplotlib import pyplot as plt
import pandas as pd


hist = pd.read_json('/content/history.json')
print(hist.keys())
params = {
          "ytick.color" : "w",
          "xtick.color" : "w",
          "axes.labelcolor" : "w",
          "axes.edgecolor" : "w"
          }
plt.rcParams.update(params)


plt.plot(hist['auc'], color='g')
plt.plot(hist['val_auc'], color='b')
plt.title('training and validation AUC', fontsize=20)
plt.legend(['training AUC', 'validation AUC'])
plt.xlabel('epochs')
plt.ylabel('AUC')
plt.show()

#plt.savefig('auc.jpg')

